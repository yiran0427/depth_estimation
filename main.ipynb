{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H3OUui-R21La"
   },
   "source": [
    "# Self-Supervised Monocular Depth Estimation using Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The purpose of this project is to investigate the self-supervised monocular depth estimation problem. There are lots of traditional approaches which extracts disparity map from binocular, motion, and multi-view stereo, but most of them require multiple viewpoints of the scene or the geometric information of the scene to be available. The supervised learning networks also achieved great success, but they suffer from the problem of insufficent training data since the ground truth disparity is hard to obtain. Therefore, the self-supervised monocular despth estimation method was proposed where only one image of the scene is required at test time, and there is no need of the ground truth target. \n",
    "\n",
    "#### In this project, we followed one of the self-supervised single image depth estimation research and implemented the network based on the result from this paper: [Unsupervised Monocular Depth Estimation with Left-Right Consistency](https://arxiv.org/abs/1609.03677v3). The authors proposed a CNN to perform single image depth estimation, despite the absence of ground truth depth data. The main innovation from this paper is the training loss which aims to enforce the consistency between the predicted left and right disparities, which will be further discussed in the following sections. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Team members and contribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Yiran Cao\n",
    "* implemented the dataloader, train and test function\n",
    "* visualization of the loss curve and resulting disparity maps\n",
    "* debugging\n",
    "\n",
    "#### Haoyu Tian\n",
    "* contribution 1\n",
    "* contribution 2\n",
    "\n",
    "#### Tengyu Cai\n",
    "* contribution 1\n",
    "* contribution 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- NumPy\n",
    "- PyTorch\n",
    "- Scikit-image\n",
    "- Torchvision\n",
    "- Matplotlib\n",
    "- Python Imaging Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mylibs.dataloader import KittiDataset\n",
    "from mylibs.transformation import ToResizeImage, ToRandomFlip, ToTensor, AugumentImagePair\n",
    "from mylibs.model import Model\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import skimage.transform\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.transforms.functional as tF\n",
    "\n",
    "from skimage.metrics import structural_similarity as ssim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_GPU = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-u0wgdb-3Mrq"
   },
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "\n",
    "We used the [KITTI driving dataset](http://www.cvlibs.net/datasets/kitti/raw_data.php) as our data for training and testing. Only data in the \"city\" category (around 30G) are used in the experiment due to the limited resources on memory and GPU. \"2011_09_26_drive_0002_sync\" is dedicated for testing only, and the rest are used as the training data. (The uploaded zip file contains only \"2011_09_26_drive_0048_sync\", and \"2011_09_26_drive_0002_sync\" as the training and testing data, under the directory `./data/train`, `./data/test`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = os.path.join(os.getcwd(), 'data/train')\n",
    "test_path = os.path.join(os.getcwd(), 'data/test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose([\n",
    "    ToResizeImage(),\n",
    "    ToRandomFlip(),\n",
    "    ToTensor(),\n",
    "    AugumentImagePair(),\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    ToResizeImage(),\n",
    "    ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loader\n",
    "\n",
    "The `KittiDataset` class takes the path to train/test dataset and loads the left and right images (depending on whether it is used for training or testing) in to a dictionary, then applies the transformations defined above. There are four disparity maps with different scales produced for each sample. The full implementation is at `./mylibs/dataloader.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = KittiDataset(train_path, 'train', transform = train_transform)\n",
    "test_set = KittiDataset(test_path, 'test', transform = test_transform)\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size = 5, shuffle = True)\n",
    "test_loader = DataLoader(test_set, batch_size = 77, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for data in train_set:\n",
    "#     left = data['left_image'][0]\n",
    "#     right = data['right_image'][0]\n",
    "#     ssim = ssim(left, right, data_range=right.max() - right.min(), multichannel=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement CNN using PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CNN has a encoder-decoder similar structure, which performs the image reconstruction. The objective is to train the model so that it can learn the disparity warp and use it to estimate the right image from the left image. Detailed information on our model architecture is presented in the figure below. <img src=\"images\\model_architecture.png\"> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(train_loader=train_loader,\n",
    "              test_loader=test_loader,\n",
    "              device='cuda' if USE_GPU else 'cpu', \n",
    "              epochs=1, \n",
    "              save_per_epoch=10, \n",
    "              img_height=256, \n",
    "              img_width=512, \n",
    "              model_path='output_model', \n",
    "              disp_path='output_disp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model and plot the loss curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Stage\n",
    "During each epoch, only the left image is passed into the network. Then the output of the model along with the original left and right image are passed to the `Loss` class for computing the training loss. The Adam optimizer is used in the experiment with a learning rate 0.0001. The trained model will be saved to a `.pt` file every `save_per_epoch=10` epochs, so that it is easier continue the training process or evaluate the trained model.\n",
    "\n",
    "### Loss Computation\n",
    "The overall loss is calculated as the weighted sum of three components, including smoothness, reconstruction, and left-right disparity consistency. The loss is computed over all four different scales for each sample.\n",
    "\n",
    "- Smoothness: The smoothness loss aims to avoid the unnecessary large disparity jumps in the prediction. We first compute the disparity gradient and then the L1 penalty on it. \n",
    "\n",
    "- Reconstruction: The reconstruction loss is calculated using a weighted sum of L1 and the structural similarity index measure (SSIM) metric. It is evaluated on the original left/right image and the predicted left/right image to measure the difference between two images. \n",
    "\n",
    "- Left-right Disparity Consistency: The left-right disparity consistency loss is to enforce that the predicted left and right disparity map should be identical. The L1 metric is used.\n",
    "\n",
    "More details in `./mylibs/loss.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the loss curve\n",
    "loss = np.array(loss)\n",
    "\n",
    "fig = plt.figure(figsize=(12,6)) \n",
    "plt.subplots_adjust(bottom=0.2,right=0.85,top=0.95) \n",
    "ax = fig.add_subplot(1,1,1)\n",
    "\n",
    "ax.clear() \n",
    "ax.set_xlabel('iterations')\n",
    "ax.set_ylabel('loss value') \n",
    "ax.set_title('Training loss curve') \n",
    "ax.plot(loss, label='training loss') \n",
    "ax.legend(loc='upper right') \n",
    "fig.canvas.draw() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the trained model on the testing dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Stage\n",
    "Testing starts from loading the model specified in function parameter. Then, only the left image is passed to the network to predict the disparity map from the trained model. We saved all disparity maps with scale 256x512 as the `.npy` file, and we have also save one version with the post-processing applied. The post-processing ...\n",
    "\n",
    "Finally, we visualize the original image, disparity, and the post-processed disparity in the following block. A random sample and the result will be displayed when running the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.test('output_model/0.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plot original image, disparity, and disparity with post-processing\n",
    "test_size = len(test_loader.dataset)\n",
    "\n",
    "for data in test_loader:\n",
    "    idx = random.randint(0, len(data['left_image']))\n",
    "    img = data['left_image'][idx].permute(1,2,0)\n",
    "\n",
    "disp = np.load('output_disp/disparities.npy')\n",
    "disp_pp = np.load('output_disp/disparities_pp.npy')\n",
    "\n",
    "disp_to_img = skimage.transform.resize(disp[idx].squeeze(), [256, 512], mode='constant')\n",
    "disp_pp_to_img = skimage.transform.resize(disp_pp[idx].squeeze(), [256, 512], mode='constant')\n",
    "\n",
    "fig = plt.figure(figsize=(16, 12))\n",
    "\n",
    "ax = fig.add_subplot(1,3,1)\n",
    "plt.title('original sample ' + str(idx))\n",
    "plt.imshow(img, cmap='plasma')\n",
    "\n",
    "ax = fig.add_subplot(1,3,2)\n",
    "plt.title('disparity without post-processing')\n",
    "plt.imshow(disp_to_img, cmap='plasma')\n",
    "\n",
    "ax = fig.add_subplot(1,3,3)\n",
    "plt.title('disparity with post-processing')\n",
    "plt.imshow(disp_pp_to_img, cmap='plasma')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Disscussion/Conclusion ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference\n",
    "\n",
    "- [Unsupervised Monocular Depth Estimation with Left-Right Consistency by Cle ́ment Godard, Oisin Mac Aodha and Gabriel J. Brostow](https://arxiv.org/abs/1609.03677v3) and its [Git Repo](https://github.com/mrharicot/monodepth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "model.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
