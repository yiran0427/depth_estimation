{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H3OUui-R21La"
   },
   "source": [
    "# Self-Supervised Monocular Depth-Estimation using Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Traditional image depth estimation approaches are mostly relied on structure from motion and multi-view stereo, where most of them require multiple viewpoints of the scene are available. There are a large number of recent studies focus on removing such limitation by applying Machine Learning based method to perform munocular depth estimation.\n",
    "\n",
    "#### In this project, we implemented one of the self-supervised single image depth estimation method based on the result from this paper: [Unsupervised Monocular Depth Estimation with Left-Right Consistency](https://arxiv.org/abs/1609.03677v3). It uses a CNN to learn to perform single image depth estimation, despite the absence of ground truth depth data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Team members and contribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Yiran Cao\n",
    "* contribution 1\n",
    "* contribution 2\n",
    "\n",
    "#### Haoyu Tian\n",
    "* contribution 1\n",
    "* contribution 2\n",
    "\n",
    "#### Tengyu Cai\n",
    "* contribution 1\n",
    "* contribution 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mylibs.dataloader import KittiDataset\n",
    "from mylibs.transformation import ToResizeImage, ToRandomFlip, ToTensor, AugumentImagePair\n",
    "from mylibs.model import Model\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import skimage.transform\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.transforms.functional as tF\n",
    "\n",
    "from skimage.metrics import structural_similarity as ssim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_GPU = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-u0wgdb-3Mrq"
   },
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We use the [KITTI driving dataset](http://www.cvlibs.net/datasets/kitti/raw_data.php) as our data for training and testing as what it used by the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = os.path.join(os.getcwd(), 'data/train')\n",
    "test_path = os.path.join(os.getcwd(), 'data/test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose([\n",
    "    ToResizeImage(),\n",
    "    ToRandomFlip(),\n",
    "    ToTensor(),\n",
    "    AugumentImagePair(),\n",
    "])\n",
    "test_transform = transforms.Compose([\n",
    "    ToResizeImage(),\n",
    "    ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = KittiDataset(train_path, 'train', transform = train_transform)\n",
    "test_set = KittiDataset(test_path, 'test', transform = test_transform)\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size = 5, shuffle = True)\n",
    "test_loader = DataLoader(test_set, batch_size = 77, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for data in train_set:\n",
    "#     left = data['left_image'][0]\n",
    "#     right = data['right_image'][0]\n",
    "#     ssim = ssim(left, right, data_range=right.max() - right.min(), multichannel=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement CNN using PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images\\model_architecture.png\"> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(train_loader=train_loader,\n",
    "              test_loader=test_loader,\n",
    "              device='cuda' if USE_GPU else 'cpu', \n",
    "              epochs=1, \n",
    "              save_per_epoch=10, \n",
    "              img_height=256, \n",
    "              img_width=512, \n",
    "              model_path='output_model', \n",
    "              disp_path='output_disp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model and plot the loss graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = model.train()\n",
    "\n",
    "fig = plt.figure(figsize=(12,6)) \n",
    "plt.subplots_adjust(bottom=0.2,right=0.85,top=0.95) \n",
    "ax = fig.add_subplot(1,1,1)\n",
    "\n",
    "loss = np.array(loss)\n",
    "\n",
    "ax.clear() \n",
    "ax.set_xlabel('iterations')\n",
    "ax.set_ylabel('loss value') \n",
    "ax.set_title('Training loss curve') \n",
    "ax.plot(loss, label='training loss') \n",
    "ax.legend(loc='upper right') \n",
    "fig.canvas.draw() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the trained model on the testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.test('output_model/0.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plot original image, disparity, and disparity with post-processing\n",
    "test_size = len(test_loader.dataset)\n",
    "\n",
    "for data in test_loader:\n",
    "    idx = random.randint(0, len(data['left_image']))\n",
    "    img = data['left_image'][idx].permute(1,2,0)\n",
    "\n",
    "disp = np.load('output_disp/disparities.npy')\n",
    "disp_pp = np.load('output_disp/disparities_pp.npy')\n",
    "\n",
    "disp_to_img = skimage.transform.resize(disp[idx].squeeze(), [256, 512], mode='constant')\n",
    "disp_pp_to_img = skimage.transform.resize(disp_pp[idx].squeeze(), [256, 512], mode='constant')\n",
    "\n",
    "fig = plt.figure(figsize=(16, 12))\n",
    "\n",
    "ax = fig.add_subplot(1,3,1)\n",
    "plt.title('original sample ' + str(idx))\n",
    "plt.imshow(img, cmap='plasma')\n",
    "\n",
    "ax = fig.add_subplot(1,3,2)\n",
    "plt.title('disparity without post-processing')\n",
    "plt.imshow(disp_to_img, cmap='plasma')\n",
    "\n",
    "ax = fig.add_subplot(1,3,3)\n",
    "plt.title('disparity with post-processing')\n",
    "plt.imshow(disp_pp_to_img, cmap='plasma')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "model.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
